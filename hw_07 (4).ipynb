{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_07.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Z3MtXKA4fw",
        "outputId": "da9b75e2-a575-4d65-d15f-a4de7d17c0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql import functions as F\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import matplotlib.pyplot\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "QgS1-AW-CUMV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Spark Session"
      ],
      "metadata": {
        "id": "wKkX8En-MNH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "I712MmXQBMJU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1 - Defining the Schema before hand"
      ],
      "metadata": {
        "id": "orCJmsILMUCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "customSchema = StructType([\n",
        "      StructField(\"Date\",StringType(),True),\n",
        "      StructField(\"Location\",StringType(),True),\n",
        "      StructField(\"MinTemp\",FloatType(),True),\n",
        "      StructField(\"MaxTemp\",FloatType(),True),\n",
        "      StructField(\"Rainfall\",FloatType(),True),\n",
        "      StructField(\"Evaporation\",StringType(),True),\n",
        "      StructField(\"Sunshine\",StringType(),True),\n",
        "      StructField(\"WindGustDir\",StringType(),True),\n",
        "      StructField(\"WindGustSpeed\",FloatType(),True),\n",
        "      StructField(\"WindDir9am\",StringType(),True),\n",
        "      StructField(\"WindDir3pm\",StringType(),True),\n",
        "      StructField(\"WindSpeed9am\",FloatType(),True),\n",
        "      StructField(\"WindSpeed3pm\",FloatType(),True),\n",
        "      StructField(\"Humidity9am\",FloatType(),True),\n",
        "      StructField(\"Humidity3pm\",FloatType(),True),\n",
        "      StructField(\"Pressure9am\",FloatType(),True),\n",
        "      StructField(\"Pressure3pm\",FloatType(),True),\n",
        "      StructField(\"Cloud9am\",FloatType(),True),\n",
        "      StructField(\"Cloud3pm\",FloatType(),True),\n",
        "      StructField(\"Temp9am\",FloatType(),True),\n",
        "      StructField(\"Temp3pm\",FloatType(),True),\n",
        "      StructField(\"RainToday\",StringType(),True),\n",
        "      StructField(\"RainTomorrow\",StringType(),True)])"
      ],
      "metadata": {
        "id": "ei8MDfp2BE4G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/weatherAUS.csv\", header = True, schema = customSchema, nullValue = 'NA')\n",
        "df = df.drop(\"Date\", \"Evaporation\",\"Sunshine\",\"Cloud9am\", \"Cloud3pm\", 'WindGustDir', 'WindGustSpeed')"
      ],
      "metadata": {
        "id": "8_bJ0yUDBFCL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping all the NA Values, I have tried imputing the NA values with the average but for some reason that dataframe kept on throwing some error. So, I am just dropping the NA values."
      ],
      "metadata": {
        "id": "WkRErELxMdiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.na.drop()"
      ],
      "metadata": {
        "id": "_Km7kVzFGf9T"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step2 - Splitting the data into train and test. Train data allocates 80 percent of the total data while test data allocate about 20 percent of the total data\n"
      ],
      "metadata": {
        "id": "C4WtRm-mMu1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train,test) = df.randomSplit([0.8,0.2])"
      ],
      "metadata": {
        "id": "8PBs7pWHC5dR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3 -\n",
        "1. Usging the StringIndexer and OneHotEncoder to convert the categorical values into binay SparseVectors\n",
        "2. Creating label column for 'RainTomorrow' in binary format\n",
        "3. Using VectorAssembler to transform the features\n"
      ],
      "metadata": {
        "id": "Dqd7OMlcNKNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categoricalColumns = [\"Location\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\"]\n",
        "stages = [] # stages in Pipeline\n",
        "\n",
        "for categoricalCol in categoricalColumns:\n",
        "    # Category Indexing with StringIndexer\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\",)\n",
        "    #stringIndexer = stringIndexer.setHandleInvalid(\"keep\")\n",
        "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    # Add stages.\n",
        "    stages += [stringIndexer, encoder]"
      ],
      "metadata": {
        "id": "TLupeRrmDGPv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRWcBT1FDP1i",
        "outputId": "a30a0e24-9af0-4dec-e84b-8c358f92f1a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StringIndexer_6d98386282b7,\n",
              " OneHotEncoder_650aa580bdf2,\n",
              " StringIndexer_23bcc2545898,\n",
              " OneHotEncoder_db38dce4208a,\n",
              " StringIndexer_0e3827627722,\n",
              " OneHotEncoder_59d5742f20a1,\n",
              " StringIndexer_bc8521dbe404,\n",
              " OneHotEncoder_e46ba23eae78]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yRn8r3i3NJqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the label column\n",
        "label_stringIdx = StringIndexer(inputCol=\"RainTomorrow\", outputCol=\"label\")\n",
        "# setHandleInvalid(\"skip\"), the indexer adds new indexes when it sees new labels.\n",
        "stages += [label_stringIdx]"
      ],
      "metadata": {
        "id": "BnZ5SK71DP9z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform all features into a vector using VectorAssembler\n",
        "numericCols = [\"MinTemp\", \"MaxTemp\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Temp9am\", \"Temp3pm\", \"Pressure9am\", \"Pressure3pm\"]\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "#assembler = assembler.setHandleInvalid(\"skip\")\n",
        "stages += [assembler]\n",
        "\n",
        "\n",
        "dtree = DecisionTreeClassifier(labelCol=\"label\", featuresCol=assembler.getOutputCol())"
      ],
      "metadata": {
        "id": "hqiD-9OZDQHl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - Fitting the Machine Learning Model"
      ],
      "metadata": {
        "id": "hww79KcWN1tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(dtree.impurity, ['gini', 'entropy'])\n",
        "    .addGrid(dtree.maxBins, [5, 10, 15])\n",
        "    .addGrid(dtree.minInfoGain, [0.0, 0.2, 0.4])\n",
        "    .addGrid(dtree.maxDepth, [3, 5, 7])\n",
        "    .build())"
      ],
      "metadata": {
        "id": "7lV7Bq9oDmwq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()"
      ],
      "metadata": {
        "id": "GCO4wzjSDokp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CrossValidator(estimator=dtree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=4)\n",
        "stages += [cv]"
      ],
      "metadata": {
        "id": "tCvD5piwDqPB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 - Creating pipeline to combine everything so far"
      ],
      "metadata": {
        "id": "2ZQopNQUN7_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Creating the pipeline with all the above steps\n",
        "pipeline = Pipeline().setStages(stages)\n",
        "pipeline_model = pipeline.fit(train)\n",
        "predictions = pipeline_model.transform(test)"
      ],
      "metadata": {
        "id": "6RZHt03tDtW8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = pipeline_model.stages[-1].bestModel"
      ],
      "metadata": {
        "id": "ZPzTNbyyDuZj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-E9aJthJD0bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_modelobj = best_model._java_obj.parent()\n",
        "\n",
        "best_modeldepth = best_modelobj.getMaxDepth()\n",
        "best_modelbins = best_modelobj.getMaxBins()\n",
        "best_modelimpurity = best_modelobj.getImpurity()\n",
        "best_modelgain = best_modelobj.getMinInfoGain()"
      ],
      "metadata": {
        "id": "3lrBCoLyDuib"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best model grid params are \")\n",
        "print(best_modeldepth)\n",
        "print(best_modelbins)\n",
        "print(best_modelimpurity)\n",
        "print(best_modelgain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiiFBcK9DuqC",
        "outputId": "94df9391-ed2e-4f9e-e153-993a06449e4a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model grid params are \n",
            "3\n",
            "5\n",
            "gini\n",
            "0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0C2vyixHRnT",
        "outputId": "30bd5b60-5399-4861-d3da-8fd06631367e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRq4yMQXHU6I",
        "outputId": "427009ed-5e85-48f6-eefe-97192993cbe9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under ROC: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator2 = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under PR: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbg1TICmHXFu",
        "outputId": "a88d15c6-8c50-48ca-ff4a-064231a3526a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under PR: 0.2266174703964304\n"
          ]
        }
      ]
    }
  ]
}